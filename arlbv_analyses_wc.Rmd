---
title: "Analysis for final, prereg. sample of ARL-BV"
author: "Daniel P"
output: html_document
---

# Overview
This markdown contains all analyses run on the final sample of ARL-BV

# Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(mc.cores=12)

path_to_project_directory <- "~/projects/ARL_bv/"
path_to_arl <- "~/projects/ARL/"
path_to_s22 <- "~/projects/spring_2022_study/"

stan_model_dir<- paste0(path_to_project_directory,"code/stan_models/final_samp_mods/")
model_out_dir <- paste0(path_to_project_directory,"output/results/stan_model_fits/final_samp_fits/")

#load custom functions
source(paste0(path_to_s22,"code/functions/s22_utilities.R")) 
source(paste0(path_to_s22,"code/functions/stan_utilities.R"))
source(paste0(path_to_arl,"code/functions/arl_utilities.R")) 
source(paste0(path_to_project_directory,"code/functions/arlbv_utilities.R")) 
source(paste0(path_to_s22,"code/functions/fit_stan_model.R"))

library(tidyverse)
library(cmdstanr)
library(loo)
library(bayesplot)
library(tidybayes)
library(lmerTest)
library(sigmoid)
library(abind)
library(future)
library(doFuture)
library(foreach)
```

Read in data, run through QC
```{r}
trials <- read.csv(paste0(path_to_project_directory,"analysis_data/trial_level_data_all_subs_2023-11-27_18_52_39.243708.csv"))
subs <- read.csv(paste0(path_to_project_directory,"analysis_data/sub_level_data_all_subs_2023-11-27_18_52_39.243708.csv"))

sub_hard_fail <- subs %>% filter(att_checks_passed < 5 |
                                 valrat_skipped_percent > .1 |
                                 late_percent > .2 |
                                 consecutive_late_choices > 9 |
                                 trials_completed < 144 |
                                 sd_valrat < .05 |
                                 percent_left > .8 |
                                 percent_left < .2 |
                                 percent_left_b1 < .1 |
                                 percent_left_b1 > .9 |
                                 percent_left_b2 < .1 |
                                 percent_left_b2 > .9 |
                                 percent_left_b3 < .1 |
                                 percent_left_b3 > .9 |
                                 answers_incorrect > 2)

#count the number of soft QC cutoffs each subject meets
subs$softs <- 0
for(i in 1:nrow(subs)){
  subs$softs[i] <- length(which(c(subs$att_checks_passed[i] == 5,subs$valrat_skipped_percent[i] > .05, subs$late_percent[i] > .15,
                                  subs$sd_valrat[i] < .1, subs$percent_left[i] > .75, subs$percent_left[i] < .25,
                                  subs$percent_left_b1[i] > .85, subs$percent_left_b1[i] < .15, subs$percent_left_b2[i] > .85, 
                                  subs$percent_left_b2[i] < .15,subs$percent_left_b3[i] > .85, subs$percent_left_b3[i] < .15,subs$answers_incorrect[i] > 1)))
}

sub_soft_fail <- filter(subs,softs >= 2) #identify those who meet more than 2 soft cutoffs

subs_to_exclude <- unique(c(sub_hard_fail$id,sub_soft_fail$id)) #get subjects who failed either set of criteria

#clean data
trials <- trials %>% filter(!(id %in% subs_to_exclude)) %>% filter(choice != "late") 
subs <- subs %>% filter(!(id %in% subs_to_exclude))

length(subs_to_exclude)/(nrow(subs)+length(subs_to_exclude)) #get percent excluded
```

Data transformations
```{r}
trials <- add_sub_indices(trials) #add subject indices to the df. These will match the indices used in Stan.
trials <- add_probe_number(trials,newcol="rat_number",val_col="valrat_z") #add rating number

#add a column with completed trial numbers - the trial indices if you ignore late trials. These will match the "t" trial numbers used in the Stan models
trials <- do.call(rbind,by(trials,trials$sub_index,add_trials_nl))
trials$overall_trial_nl <- 1:nrow(trials) #get the overall trial number ignoring late trials and collapsing across subjects

#Create indices from 1:n_f for each fractal image. To do this, first create a mini-df with one column having all the fA_img values and the other two
#columns having indices for fA and fB. This assumes that every fA_img is paired with a unique fB_img.
f_index_df <- data.frame(fA_img = unique(trials$fA_img),fA_ix = 1:length(unique(trials$fA_img)),fB_ix = (1:length(unique(trials$fA_img))+length(unique(trials$fA_img))))
trials <- left_join(trials,f_index_df,by="fA_img")

#get the chosen fractal index
trials <- trials %>% mutate(chosen_frac = ifelse(choice == "fA",fA_ix,fB_ix))
trials <- trials %>% mutate(unchosen_frac = ifelse(choice == "fA",fB_ix,fA_ix))
```

# Data review

```{r}
dr_vars <- c("valreg_rsq","bv_b","fres_b","fres_bv_ratio")
plot_sub_level_vars(subs,dr_vars,path_to_project_directory,type="dr") #create and save plot grids
```

```{r}
subs$age <- as.numeric(subs$age)
age_no_skips <- filter(subs,age < 900)
plot_hist(age_no_skips,"age") 
```

```{r}
sum(subs$gender=="Male")
```

```{r}
sum(subs$gender=="Female")
```

```{r}
sum(subs$gender=="Non-binary")
```

```{r}
sub_age <- subs %>% filter(age < 80)
mean(sub_age$age)
sd(sub_age$age)
```

```{r}
trials$pair_pres_num <- rep(c(1:48),323*3)
percentage_plots(trials,"stay")
```

# Preregistered analyses

Fit first couple of models
```{r}
one_q <- fit_stan_model(stan_file=paste0(stan_model_dir,"one_q.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

Check diagnostics
```{r}
one_q$diagnostics
```

```{r}
two_q$diagnostics
```
Ok

Compare
```{r}
fsml_compare(one_q,two_q)
```
Looks like two_q fits better

Does this show up in the learning and forgetting rates?
```{r}
twoq_lrn_dc_draws <- get_draws("two_q",model_out_dir=model_out_dir,vars=c("lrn_fr_mu","lrn_nf_mu","dcy_fr_mu","dcy_nf_mu"))
mcmc_areas(
  lfsm_draws,
  area_method = "scaled height",
  prob = 0.5,
  prob_outer = .95, 
  point_est = "median"
)
```

```{r}
sigmoid(filt_sum(two_q$sum,"lrn_fr_mu")$median)
sigmoid(filt_sum(two_q$sum,"lrn_nf_mu")$median)
(sigmoid(twoq_lrn_dc_draws[,,"lrn_fr_mu"]) - sigmoid(twoq_lrn_dc_draws[,,"lrn_nf_mu"])) > 0
```

Indeed, the learning rate is higher for fres than no-fres trials, as predicted.

Fit winner with side bias
```{r}
two_q_sidebias <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_sidebias.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 4,
                                   n_t = 144)
```

Compare
```{r}
fsml_compare(two_q,two_q_sidebias)
```

The model with side bias fits better. HOWEVER, you initially thought that the model without side bias fit
better, and you proceeded on that assumption. Below are analyses based on that faulty assumption. Under "Side
Bias reanalysis", you rerun analyses to reflect the fact that the model should include side bias, after all.

Testing effects of reward and affect associations on choice in two_q model
```{r}
twoq_aff <- get_draws("two_q",vars=c("scd_aff_fr_sens_mu"))
quantile(twoq_aff,c(.025,.5,.975))
mean(twoq_aff > 0)

twoq_rew <- get_draws("two_q",vars=c("scd_rew_fr_sens_mu"))
quantile(twoq_rew,c(.025,.5,.975))
mean(twoq_rew > 0)
```


# Exploratory analyses

## Momentum model
Momentum model
```{r}
mom_rat <- fit_stan_model(stan_file=paste0(stan_model_dir,"mom_rat.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
mom_terms <- get_draws("mom_rat",model_out_dir=model_out_dir,vars=c("x_sens_mu","aff_fr_sens_mu","B_auto_mu"))
mcmc_areas(
  mom_terms,
  pars = "x_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
total_mom <- mom_terms[,,"x_sens_mu"] + mom_terms[,,"aff_fr_sens_mu"]*mom_terms[,,"B_auto_mu"]
mcmc_areas(
  total_mom,
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

## Separating modeled and residual affect

Predict RL using model-predicted affect, and residual affect, with a fuller affect model
```{r}
mod_aff_resid <- fit_stan_model(stan_file=paste0(stan_model_dir,"mod_aff_resid.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
mom_sens_draws <- get_draws("mod_aff_resid",model_out_dir=model_out_dir,vars=c("aff_fr_sens_mu","rew_fr_sens_mu","resid_fr_sens_mu"))
mcmc_areas(
  mom_sens_draws,
  pars = "aff_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

Is this a stronger effect than what we had before?
```{r}
twoq_int_draws <- get_draws("two_q",model_out_dir=model_out_dir,vars=c("aff_fr_sens_mu"))

m_vs_rat <- mom_sens_draws[,,"aff_fr_sens_mu"] - twoq_int_draws[,,"aff_fr_sens_mu"]
quantile(m_vs_rat,probs=c(.01,.025,.05,.5,.95,.975,.99))

m_vs_resid <- mom_sens_draws[,,"aff_fr_sens_mu"] - mom_sens_draws[,,"resid_fr_sens_mu"]
quantile(m_vs_resid,probs=c(.01,.025,.05,.5,.95,.975,.99))
```


```{r}
mcmc_areas(
  mom_sens_draws,
  pars = "rew_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

```{r}
mcmc_areas(
  mom_sens_draws,
  pars = "resid_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

```{r}
filt_sum(mod_aff_resid$sum,"mu")
```

Now, breaking things down further: separately estimating effects of modeled affect, residual affect, and nuisance variation.
```{r}
breakdown <- fit_stan_model(stan_file=paste0(stan_model_dir,"breakdown.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144,
                                   iter_sampling = 6000)
```

```{r}
filt_sum(breakdown$sum,"mu")
filt_sum(mod_aff_resid$sum,"mu")
```
Looks like it was worthwhile to break the result down like this - it resulted in much stronger estimates for aff_fres and allowed the effect of nuisance affect to be what it is.

```{r}
fsml_compare(breakdown,mod_aff_resid)
```

```{r}
breakdown_ma_sens <- get_draws("breakdown",vars=c("scd_ma_fr_sens_mu"))
quantile(breakdown_ma_sens,probs=c(.025,.5,.975))
breakdown_rew_sens <- get_draws("breakdown",vars=c("scd_rew_fr_sens_mu"))
quantile(breakdown_rew_sens,probs=c(.025,.5,.975))
quantile(breakdown_ma_sens - breakdown_rew_sens,probs=c(.025,.5,.975))
mean(breakdown_ma_sens > breakdown_rew_sens)
breakdown_resid_sens <- get_draws("breakdown",vars=c("scd_resid_fr_sens_mu"))
quantile(breakdown_resid_sens,probs=c(.025,.5,.975))
```


Next, just running a model with separate effects of model-predicted and residual valence on choice, not including
the orthogonal nuisance affect, for simplicity.
```{r}
mod_resid_sep <- fit_stan_model(stan_file=paste0(stan_model_dir,"mod_resid_sep.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

Rerunning this model with more iterations, and renaming it "final"
```{r}
final <- fit_stan_model(stan_file=paste0(stan_model_dir,"final.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144,
                                   skip=c("check_csvs","loo"),
                                   iter_sampling = 6000)
```

Getting effect estimates from the "final" model
```{r}
final_Bbv <- get_draws("final",vars=c("B_bv_fr_mu"))
final_Brew <- get_draws("final",vars=c("B_rew_fr_mu"))
final_rew <- get_draws("final",vars=c("scd_rew_fr_sens_mu"))
final_aff <- get_draws("final",vars=c("scd_aff_fr_sens_mu"))
final_resid <- get_draws("final",vars=c("scd_resid_fr_sens_mu"))
val_mod_rsq <- get_draws("final",vars=c("val_mod_rsq"))
```

```{r}
trials_fres <- filter(trials,show_fres==1)
final_Bbv_scd <- sd(trials_fres$box_val)*final_Bbv
quantile(final_Bbv_scd,c(.025,.5,.975))
mean(final_Bbv_scd < 1)
```

```{r}
final_Brew_scd <- sd(trials_fres$out)*final_Brew
quantile(final_Brew_scd,c(.025,.5,.975))
mean(final_Brew_scd > 1)
```


```{r}
quantile(final_aff-final_rew,c(.025,.5,.975))
mean(final_aff > final_rew)
quantile(final_aff,c(.025,.5,.975))
mean(final_aff > 0)
quantile(final_rew,c(.025,.5,.975))
mean(final_rew > 0)
```

```{r}
quantile(final_resid,c(.025,.5,.975))
mean(final_resid > 0)

quantile(final_aff-final_resid,c(.025,.5,.975))
mean(final_aff > final_resid)
```

getting r-squared value
```{r}
median(val_mod_rsq)
```

# Side bias reanalysis

After re-doing the model comparison between the one_q, two_q, and two_q_sidebias models, I realized that
two_q_sidebias actually fit better. So below you rerun analyses accordingly.

## New model
```{r}
final_ls <- fit_stan_model(stan_file=paste0(stan_model_dir,"final_ls.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144,
                                   skip=c("check_csvs","loo"),
                                   iter_sampling = 6000)
```

```{r}
scd_aff_fr_sens_mu <- get_draws("final_ls",vars=c("scd_aff_fr_sens_mu"),model_out_dir=model_out_dir)
scd_rew_fr_sens_mu <- get_draws("final_ls",vars=c("scd_rew_fr_sens_mu"),model_out_dir=model_out_dir)
quantile(scd_aff_fr_sens_mu-scd_rew_fr_sens_mu,c(.025,.5,.975))
mean(scd_aff_fr_sens_mu > scd_rew_fr_sens_mu)
```

I think you redid some of the stats but didn't put them in the markdown. Oh well. These analyses are in the
manuscript repo, at least.

Fitting at 1000 iterations so it can be compared to two_q_sidebias, and so it finishes running quicker
```{r}
final_ls_test <- fit_stan_model(stan_file=paste0(stan_model_dir,"final_ls_test.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

```{r}
fsml_compare(final_ls_test,two_q_sidebias)
```

## Getting results from two_q_sidebias

```{r}
aff_twoq_side <- get_draws("two_q_sidebias",vars=c("scd_aff_fr_sens_mu"),model_out_dir=model_out_dir)
rew_twoq_side <- get_draws("two_q_sidebias",vars=c("scd_rew_fr_sens_mu"),model_out_dir=model_out_dir)

cat("Effect of reward associations on choice\n")
quantile(rew_twoq_side,c(.50,.025,.975))
round(mean(rew_twoq_side > 0) * 100,1)

cat("Effect of affect associations on choice\n")
quantile(aff_twoq_side,c(.50,.025,.975))
round(mean(aff_twoq_side > 0) * 100,1)
```

# Regressing valence and reward onto choice

Do a model-free test of whether valence and reward both predict choice
```{r}
trials_fres <- filter(trials,show_fres == 1)
trials_fres_scale <- trials_fres %>% mutate(valrat_z=scale(valrat_z),out=scale(out))
stay_fit <- lmer(stay ~ valrat_z + out + (valrat_z + out | sub_index),trials_fres_scale)
summary(stay_fit)
confint(stay_fit, parm = c("valrat_z","out"))
```


#Utility vs. reinforcement test

Finally, testing the hypothesis that valence reflects utility rather than reinforcement in this task.

final_ls but affect is reinforcement rather than utility
```{r}
flt_pe <- fit_stan_model(stan_file=paste0(stan_model_dir,"flt_pe.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

```{r}
flt <- read_fsml("final_ls_test",model_out_dir=model_out_dir)
```

```{r}
flt_pe$loo
```

```{r}
fsml_compare(flt_pe,flt)
```
No real difference here

Now doing the comparison if you only predict choice with modeled affect.
```{r}
mod_aff <- fit_stan_model(stan_file=paste0(stan_model_dir,"mod_aff.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

```{r}
mod_aff_pe <- fit_stan_model(stan_file=paste0(stan_model_dir,"mod_aff_pe.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

```{r}
fsml_compare(mod_aff,mod_aff_pe)
```
Still no meaningful difference.


Now, testing if BV impacts choice more as utility or reinforcement
```{r}
rew_bv <- fit_stan_model(stan_file=paste0(stan_model_dir,"rew_bv.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

```{r}
rew_bv_pe <- fit_stan_model(stan_file=paste0(stan_model_dir,"rew_bv_pe.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

```{r}
fsml_compare(rew_bv,rew_bv_pe)
```

```{r}
view(filt_sum(rew_bv_pe$sum,"mu"))
```
No difference here either


Now doing the same thing but with r-BV instead of just BV
```{r}
rew_min_bv <- fit_stan_model(stan_file=paste0(stan_model_dir,"rew_min_bv.stan"),
                             model_out_dir=model_out_dir,
                             raw_data=trials,
                             study = "arlbv",
                             chains = 3,
                             n_t = 144)
```

```{r}
rew_min_bv_pe <- fit_stan_model(stan_file=paste0(stan_model_dir,"rew_min_bv_pe.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
fsml_compare(rew_min_bv,rew_min_bv_pe)
```
Still no difference.

Finally, run these tests with raw valence ratings.
```{r}
two_q_sidebias_pe <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_sidebias_pe.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 4,
                                   n_t = 144)
```

```{r}
fsml_compare(two_q_sidebias_pe,two_q_sidebias)
```
Okay, so when using raw ratings, the model fits better assuming that valence is utility. Now, re-testing without
Q-learning.

```{r}
two_q_sidebias_pe_aonly <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_sidebias_pe_aonly.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
two_q_sidebias_aonly <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_sidebias_aonly.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
fsml_compare(two_q_sidebias_pe_aonly,two_q_sidebias_aonly)
```
Same result, but more dramatic.


Finally, testing whether the effect of valence on choice can be seen to reflect an exponential decay in raw
regression coefficients.

```{r}
trials_diffs <- do.call(rbind,by(trials,trials$sub_index,sub_past_diffs))

diff_reg_fit_1 <- lmer(stay ~ valrat_z_diff_1 + out_diff_1 + (valrat_z_diff_1 + out_diff_1 | sub_index),trials_diffs)
diff_reg_fit_1 <- lm(stay ~ valrat_z_diff_1 + out_diff_1,trials_diffs)

summary(diff_reg_fit_1)

diff_reg_fit_3 <- lmer(stay ~ valrat_z_diff_1 + out_diff_1 + valrat_z_diff_2 + out_diff_2 + valrat_z_diff_3 + out_diff_3 +
                              (valrat_z_diff_1 + out_diff_1 + valrat_z_diff_2 + out_diff_2 + valrat_z_diff_3 + out_diff_3 | sub_index),trials_diffs)

diff_reg_fit_3 <- lm(stay ~ valrat_z_diff_1 + out_diff_1 + valrat_z_diff_2 + out_diff_2 + valrat_z_diff_3 + out_diff_3,trials_diffs)
summary(diff_reg_fit_3)$coefficients
coef_values <- coefficients[2:4, 1]
```

```{r}
library(car)
linearHypothesis(diff_reg_fit_3,"valrat_z_diff_1 - valrat_z_diff_2 = 0")
linearHypothesis(diff_reg_fit_3,"valrat_z_diff_1 - valrat_z_diff_3 = 0")
```

# Get parameter estimates from final model not reported in the main article
```{r}
# Get info on the posterior distribution for all relevant parameters
unreported_params <- "rew_nf_sens_mu|aff_nf_sens_mu|resid_nf_sens_mu|csens_mu|dcy_fr_mu|dcy_nf_mu|lrn_fr_mu|ls_bias_mu|lrn_nf_mu|lrn_c_mu|B_0_mu|B_rew_nf_mu|B_q_fr_mu|B_q_nf_mu|B_pwqd_fr_mu|B_pwqd_nf_mu|B_auto_mu"
up_tab <- select(filt_sum(mod_resid$sum,unreported_params),variable,median,q5,q95)
up_tab[2,3,4] <- as.numeric(up_tab[2,3,4])

# Replace choice predictors with scaled effects
scd_rew_nf_sens_mu <- get_draws("mod_resid",vars=c("scd_rew_nf_sens_mu"), model_out_dir = model_out_dir)
up_tab[1,2:4] <- t(quantile(scd_rew_nf_sens_mu,c(.50,.05,.95)))
scd_aff_nf_sens_mu <- get_draws("mod_resid",vars=c("scd_aff_nf_sens_mu"), model_out_dir = model_out_dir)
up_tab[2,2:4] <- t(quantile(scd_aff_nf_sens_mu,c(.50,.05,.95)))
scd_resid_nf_sens_mu <- get_draws("mod_resid",vars=c("scd_resid_nf_sens_mu"), model_out_dir = model_out_dir)
up_tab[3,2:4] <- t(quantile(scd_resid_nf_sens_mu,c(.50,.05,.95)))
scd_csens_mu <- get_draws("mod_resid",vars=c("scd_csens_mu"), model_out_dir = model_out_dir)
up_tab[5,2:4] <- t(quantile(scd_csens_mu,c(.50,.05,.95)))

up_tab[6:10,2:4] <- sigmoid(up_tab[6:10,2:4]) # Transform learning/decay rates to their original scales

up_tab[,c("median","q5","q95")] <- round(up_tab[,c("median","q5","q95")],2)
write.csv(up_tab,paste0(fig_dir,"unrep_params_s3.csv")) # This CSV will be converted into a figure
```

# Vector-based analysis of choice
```{r}
rew_bv_final <- fit_stan_model(stan_file=paste0(stan_model_dir,"rew_bv_final.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144,
                                   skip=c("check_csvs","loo"))
```


Generate length estimates
```{r}
rb_draws <- get_draws("rew_bv_final",vars=c("rew_fr_sens_mu","bv_fr_sens_mu","B_rew_fr_mu","B_bv_fr_mu"))
vec_ws <- apply(rb_draws,c(1,2), rew_bv_vec_optim) # Estimate vec lengths
vec_ws_org <- aperm(vec_ws, c(2,3,1)) # Get the dimensions back in order

#Get the Manhattan norm ratios for these lengths 
mn_data <- apply(rb_draws[,,c(1:2)],c(1,2),function(x) sum(abs(x))) # Get Manhattan norms
comb_array <- abind(vec_ws_org,mn_data,along=3) 
vec_ws_norm <- apply(comb_array,c(1,2), get_ports) 

#Arrange and name properly
vec_ws_norm_org <- aperm(vec_ws_norm,c(2,3,1)) 
dimnames(vec_ws_norm_org)[[3]] <- c("rew","aff","resid") 
```

Get 95% credible intervals and pd values for all vectors
```{r}
cat("rew\n")
quantile(vec_ws_norm_org[,,"rew"],c(.50,.025,.975))
cat(paste0("pd:", round(mean(vec_ws_norm_org[,,"rew"] > 0) * 100,1)))

cat("\n\naff\n")
quantile(vec_ws_norm_org[,,"aff"],c(.50,.025,.975))
cat(paste0("pd:", round(mean(vec_ws_norm_org[,,"aff"] > 0) * 100,1)))
```

Create interval plots for Manhattan norm ratios
```{r}
variable_intervals<- create_interval_plot(arr = vec_ws_norm_org,names = c("resid","aff","rew"),
                                          xmin = -.012,xmax = 1) +
                        theme(panel.grid.major.x = element_line(color = "#DCDCDC", size = .47),
                              panel.background = element_rect(fill = "white", color = NA),
                              axis.text.x = element_blank(),
                              axis.text.y=element_blank())

#ggsave(paste0(fig_dir,"vec_lens_s3.pdf"),variable_intervals,width=1.8,height=1.35)
```

# Testing for spurious positive effects of reward associations
Here, we consider whether the observed effect of reward associations on choice could be due to its collinearity
with affect assocations.

First, we fit a model and simulate data in which only affect associations influence choice
```{r}
# aff_only_s3 <- fit_stan_model(stan_file=paste0(stan_model_dir,"aff_only_s3.stan"),
#                                    model_out_dir=model_out_dir,
#                                    raw_data=trials,
#                                    study = "arlbv",
#                                    n_t = 144,
#                                    skip=c("check_csvs","loo"))
aff_only_s3 <- read_fsml("aff_only_s3",model_out_dir=model_out_dir)
```

For each simulated dataset, we estimate the effects of Q and A on choice, using an analysis method that 
efficiently approximates the model reported in the manuscript
```{r}
# Get necessary data
trials_bqba_sim <- trials %>% select(sub_index,block,overall_trial_nl,fA_ix,fB_ix,rat_number,show_fres,out,box_val)

# Add parameter means needed for approximate data analysis
trials_bqba_sim <- add_param_means(aff_only_s3$sum,"lrn_fr",trials_bqba_sim)
trials_bqba_sim <- add_param_means(aff_only_s3$sum,"lrn_nf",trials_bqba_sim)
trials_bqba_sim <- add_param_means(aff_only_s3$sum,"dcy_fr",trials_bqba_sim)
trials_bqba_sim <- add_param_means(aff_only_s3$sum,"dcy_nf",trials_bqba_sim)
trials_bqba_sim <- add_param_means(aff_only_s3$sum,"lrn_c",trials_bqba_sim)

# Get simulated data
sim_choice <- get_draws(model="aff_only_s3",vars=c("sim_choice"),model_out_dir=model_out_dir)
sim_rat <- get_draws("aff_only_s3",vars=c("sim_rat"),model_out_dir=model_out_dir)
sim_prat <- get_draws("aff_only_s3",vars=c("sim_prat"),model_out_dir=model_out_dir)



plan(multisession, workers = 12)
registerDoFuture()

sim_effs <- data.frame("bQs"=c(),"bAs"=c())
for(chunk in 1:2){
  # Divide the simulations into 8 chunks, to avoid taxing memory
  iters <- ((chunk-1)*12+1):(chunk*12)
  sim_choice_chunk <- sim_choice[iters,,]
  sim_prat_chunk <- sim_prat[iters,,]
  sim_rat_chunk <- sim_rat[iters,,]
  chunk_effs <- foreach(i = 1:12, .combine = rbind) %dopar% {
    iter_effs <- data.frame(matrix(0,ncol=2,nrow=4))
    colnames(iter_effs) <- c("bQs","bAs")
    for(c in 1:4){
      # Add simulated choices and ratings to the data
      sim_trials <- trials_bqba_sim %>% 
                      mutate(choice=as.vector(sim_choice_chunk[i,c,])) %>%
                      mutate(prat=as.vector(sim_prat_chunk[i,c,])) %>%
                      left_join(data.frame(valrat_z = as.vector(sim_rat_chunk[i,c,]),rat_number = 1:dim(sim_rat_chunk)[3]),
                                by="rat_number") %>%
                      mutate(chosen_frac = ifelse(choice == 1,fA_ix,fB_ix)) %>%
                      mutate(unchosen_frac = ifelse(choice == 1,fB_ix,fA_ix))
      # Get estimated effects of Q and A on choice
      est_eff <- est_bq_ba(sim_trials)
      iter_effs$bQs[c] <- est_eff$bQ
      iter_effs$bAs[c] <- est_eff$bA
    }
    return(iter_effs)
  }
  sim_effs <- rbind(sim_effs,chunk_effs)
}
```


Next, we run these same approximate analyses on the real data
```{r}

```

Finally, we plot the simulated and actual effects for comparison
```{r}

```

