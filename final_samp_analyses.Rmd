---
title: "Analysis for final, prereg. sample of ARL-BV"
author: "Daniel P"
output: html_document
---

Set up
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(mc.cores=12)

path_to_project_directory <- "~/projects/ARL_bv/"
path_to_arl <- "~/projects/ARL/"
path_to_s22 <- "~/projects/spring_2022_study/"

stan_model_dir<- paste0(path_to_project_directory,"code/stan_models/final_samp_mods/")
model_out_dir <- paste0(path_to_project_directory,"output/results/stan_model_fits/final_samp_fits/")

#load custom functions
source(paste0(path_to_s22,"code/functions/s22_utilities.R")) 
source(paste0(path_to_s22,"code/functions/stan_utilities.R"))
source(paste0(path_to_arl,"code/functions/arl_utilities.R")) 
source(paste0(path_to_project_directory,"code/functions/arlbv_utilities.R")) 
source(paste0(path_to_s22,"code/functions/fit_stan_model.R"))

library(tidyverse)
library(cmdstanr)
library(loo)
library(bayesplot)
library(tidybayes)
library(lme4)
```

Read in data, run through QC
```{r}
trials <- read.csv(paste0(path_to_project_directory,"analysis_data/trial_level_data_all_subs_2023-11-27_18_52_39.243708.csv"))
subs <- read.csv(paste0(path_to_project_directory,"analysis_data/sub_level_data_all_subs_2023-11-27_18_52_39.243708.csv"))

sub_hard_fail <- subs %>% filter(att_checks_passed < 5 |
                                 valrat_skipped_percent > .1 |
                                 late_percent > .2 |
                                 consecutive_late_choices > 9 |
                                 trials_completed < 144 |
                                 sd_valrat < .05 |
                                 percent_left > .8 |
                                 percent_left < .2 |
                                 percent_left_b1 < .1 |
                                 percent_left_b1 > .9 |
                                 percent_left_b2 < .1 |
                                 percent_left_b2 > .9 |
                                 percent_left_b3 < .1 |
                                 percent_left_b3 > .9 |
                                 answers_incorrect > 2)

#count the number of soft QC cutoffs each subject meets
subs$softs <- 0
for(i in 1:nrow(subs)){
  subs$softs[i] <- length(which(c(subs$att_checks_passed[i] == 5,subs$valrat_skipped_percent[i] > .05, subs$late_percent[i] > .15,
                                  subs$sd_valrat[i] < .1, subs$percent_left[i] > .75, subs$percent_left[i] < .25,
                                  subs$percent_left_b1[i] > .85, subs$percent_left_b1[i] < .15, subs$percent_left_b2[i] > .85, 
                                  subs$percent_left_b2[i] < .15,subs$percent_left_b3[i] > .85, subs$percent_left_b3[i] < .15,subs$answers_incorrect[i] > 1)))
}

sub_soft_fail <- filter(subs,softs >= 2) #identify those who meet more than 2 soft cutoffs

subs_to_exclude <- unique(c(sub_hard_fail$id,sub_soft_fail$id)) #get subjects who failed either set of criteria

#clean data
trials <- trials %>% filter(!(id %in% subs_to_exclude)) %>% filter(choice != "late") 
subs <- subs %>% filter(!(id %in% subs_to_exclude))

length(subs_to_exclude)/(nrow(subs)+length(subs_to_exclude)) #get percent excluded
```

Data transformations
```{r}
trials <- add_sub_indices(trials) #add subject indices to the df. These will match the indices used in Stan.
trials <- add_probe_number(trials,newcol="rat_number",val_col="valrat_z") #add rating number

#add a column with completed trial numbers - the trial indices if you ignore late trials. These will match the "t" trial numbers used in the Stan models
trials <- do.call(rbind,by(trials,trials$sub_index,add_trials_nl))
trials$overall_trial_nl <- 1:nrow(trials) #get the overall trial number ignoring late trials and collapsing across subjects

#Create indices from 1:n_f for each fractal image. To do this, first create a mini-df with one column having all the fA_img values and the other two
#columns having indices for fA and fB. This assumes that every fA_img is paired with a unique fB_img.
f_index_df <- data.frame(fA_img = unique(trials$fA_img),fA_ix = 1:length(unique(trials$fA_img)),fB_ix = (1:length(unique(trials$fA_img))+length(unique(trials$fA_img))))
trials <- left_join(trials,f_index_df,by="fA_img")

#get the chosen fractal index
trials <- trials %>% mutate(chosen_frac = ifelse(choice == "fA",fA_ix,fB_ix))
trials <- trials %>% mutate(unchosen_frac = ifelse(choice == "fA",fB_ix,fA_ix))
```

Some additional data review

```{r}
dr_vars <- c("valreg_rsq","bv_b","fres_b","fres_bv_ratio")
plot_sub_level_vars(subs,dr_vars,path_to_project_directory,type="dr") #create and save plot grids
```

```{r}
subs$age <- as.numeric(subs$age)
age_no_skips <- filter(subs,age < 900)
plot_hist(age_no_skips,"age") 
```

```{r}
sum(subs$gender=="Male")
```

```{r}
sum(subs$gender=="Female")
```

```{r}
sum(subs$gender=="Non-binary")
```

```{r}
sub_age <- subs %>% filter(age < 80)
mean(sub_age$age)
sd(sub_age$age)
```

```{r}
trials$pair_pres_num <- rep(c(1:48),323*3)
percentage_plots(trials,"stay")
```

# Preregistered analyses

Fit first couple of models
```{r}
one_q <- fit_stan_model(stan_file=paste0(stan_model_dir,"one_q.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
two_q <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144,
                                   iter_sampling = 5000)
```

Check diagnostics
```{r}
one_q$diagnostics
```

```{r}
two_q$diagnostics
```
Ok

Compare
```{r}
fsml_compare(one_q,two_q)
```
Looks like two_q fits better

Does this show up in the learning and forgetting rates?
```{r}
twoq_lrn_dc_draws <- get_draws("two_q",model_out_dir=model_out_dir,vars=c("lrn_fr_mu","lrn_nf_mu","dcy_fr_mu","dcy_nf_mu"))
mcmc_areas(
  lfsm_draws,
  area_method = "scaled height",
  prob = 0.5,
  prob_outer = .95, 
  point_est = "median"
)
```

```{r}
sigmoid(filt_sum(two_q$sum,"lrn_fr_mu")$median)
sigmoid(filt_sum(two_q$sum,"lrn_nf_mu")$median)
(sigmoid(twoq_lrn_dc_draws[,,"lrn_fr_mu"]) - sigmoid(twoq_lrn_dc_draws[,,"lrn_nf_mu"])) > 0
```

Indeed, the learning rate is higher for fres than no-fres trials, as predicted.

Fit winner with side bias
```{r}
two_q_sidebias <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_sidebias.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

Compare
```{r}
fsml_compare(two_q,two_q_sidebias)
```
Not clearly better. Stick with two_q

# Hypothesis tests

Posterior for effect of affect on choice
```{r}
twoq_int_draws <- get_draws("two_q",model_out_dir=model_out_dir,vars=c("aff_fr_sens_mu","rew_fr_sens_mu","B_rew_fr_mu","B_rew_nf_mu","B_bv_fr_mu","B_auto_mu"))

mcmc_areas(
  twoq_int_draws,
  pars = "aff_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

Posterior for effect of reward on choice
```{r}
mcmc_areas(
  twoq_int_draws,
  pars = "rew_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

```{r}
two_q_scds <- get_draws("two_q",model_out_dir=model_out_dir,vars = c("scd_aff_fr_sens_mu","scd_rew_fr_sens_mu","scd_csens_mu"))
quantile(two_q_scds[,,"scd_aff_fr_sens_mu"],c(.025,.5,.975))
quantile(two_q_scds[,,"scd_rew_fr_sens_mu"],c(.025,.5,.975))
quantile(two_q_scds[,,"scd_csens_mu"],c(.025,.5,.975))
```

# Other model parameters

```{r}
two_q <- read_fsml("two_q",model_out_dir=model_out_dir)
view(filt_sum(two_q$sum,"mu"))
```

# Exploratory analyses

Predict RL using model-predicted affect, and residual affect, with a fuller affect model
```{r}
mod_aff_resid <- fit_stan_model(stan_file=paste0(stan_model_dir,"mod_aff_resid.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
mom_sens_draws <- get_draws("mod_aff_resid",model_out_dir=model_out_dir,vars=c("aff_fr_sens_mu","rew_fr_sens_mu","resid_fr_sens_mu"))
mcmc_areas(
  mom_sens_draws,
  pars = "aff_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

Is this a stronger effect than what we had before?
```{r}
twoq_int_draws <- get_draws("two_q",model_out_dir=model_out_dir,vars=c("aff_fr_sens_mu"))

m_vs_rat <- mom_sens_draws[,,"aff_fr_sens_mu"] - twoq_int_draws[,,"aff_fr_sens_mu"]
quantile(m_vs_rat,probs=c(.01,.025,.05,.5,.95,.975,.99))

m_vs_resid <- mom_sens_draws[,,"aff_fr_sens_mu"] - mom_sens_draws[,,"resid_fr_sens_mu"]
quantile(m_vs_resid,probs=c(.01,.025,.05,.5,.95,.975,.99))
```


```{r}
mcmc_areas(
  mom_sens_draws,
  pars = "rew_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```
```{r}
mcmc_areas(
  mom_sens_draws,
  pars = "resid_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```
```{r}
filt_sum(mod_aff_resid$sum,"mu")
```

calculate R squared for valence prediction from this
```{r}
```


Momentum model
```{r}
mom_rat <- fit_stan_model(stan_file=paste0(stan_model_dir,"mom_rat.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
mom_terms <- get_draws("mom_rat",model_out_dir=model_out_dir,vars=c("x_sens_mu","aff_fr_sens_mu","B_auto_mu"))
mcmc_areas(
  mom_terms,
  pars = "x_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
total_mom <- mom_terms[,,"x_sens_mu"] + mom_terms[,,"aff_fr_sens_mu"]*mom_terms[,,"B_auto_mu"]
mcmc_areas(
  total_mom,
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

Breaking down effects of affect rating into trial-related, nuisance variation, and residual
```{r}
breakdown <- fit_stan_model(stan_file=paste0(stan_model_dir,"breakdown.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144,
                                   iter_sampling = 6000)
```

```{r}
filt_sum(breakdown$sum,"mu")
filt_sum(mod_aff_resid$sum,"mu")
```
Looks like it was worthwhile to break the result down like this - it resulted in much stronger estimates for aff_fres and allowed nuisance to be what it is.
```{r}
fsml_compare(breakdown,mod_aff_resid)
```

```{r}
breakdown_ma_sens <- get_draws("breakdown",vars=c("scd_ma_fr_sens_mu"))
quantile(breakdown_ma_sens,probs=c(.025,.5,.975))
breakdown_rew_sens <- get_draws("breakdown",vars=c("scd_rew_fr_sens_mu"))
quantile(breakdown_rew_sens,probs=c(.025,.5,.975))
quantile(breakdown_ma_sens - breakdown_rew_sens,probs=c(.025,.5,.975))
mean(breakdown_ma_sens > breakdown_rew_sens)
breakdown_resid_sens <- get_draws("breakdown",vars=c("scd_resid_fr_sens_mu"))
quantile(breakdown_resid_sens,probs=c(.025,.5,.975))

```



```{r}
temp_breakdown <- fit_stan_model(stan_file=paste0(stan_model_dir,"temp_breakdown.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144,
                                   chains = 3)
```

# Regressing valence and reward onto choice


```{r}
trials_fres <- filter(trials,show_fres == 1)
trials_fres_scale <- trials_fres %>% mutate(valrat_z=scale(valrat_z),out=scale(out))
stay_fit <- lmer(stay ~ valrat_z + out + (valrat_z + out | sub_index),trials_fres_scale)
summary(stay_fit)
```

```{r}
# trials_diffs <- do.call(rbind,by(trials,trials$sub_index,sub_past_diffs))
# 
# diff_reg_fit_1 <- lmer(stay ~ valrat_z_diff_1 + out_diff_1 + (valrat_z_diff_1 + out_diff_1 | sub_index),trials_diffs)
# diff_reg_fit_1 <- lm(stay ~ valrat_z_diff_1 + out_diff_1,trials_diffs)
# 
# summary(diff_reg_fit_1)
# 
# diff_reg_fit_3 <- lmer(stay ~ valrat_z_diff_1 + out_diff_1 + valrat_z_diff_2 + out_diff_2 + valrat_z_diff_3 + out_diff_3 + 
#                               (valrat_z_diff_1 + out_diff_1 + valrat_z_diff_2 + out_diff_2 + valrat_z_diff_3 + out_diff_3 | sub_index),trials_diffs)
# 
# diff_reg_fit_3 <- lm(stay ~ valrat_z_diff_1 + out_diff_1 + valrat_z_diff_2 + out_diff_2 + valrat_z_diff_3 + out_diff_3,trials_diffs)
# summary(diff_reg_fit_3)
# 
# summary(diff_reg_fit_3)
```

#Utility vs. reinforcement test

```{r}
two_q_pe <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_pe.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
two_q_pe_comp <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_pe_comp.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```
