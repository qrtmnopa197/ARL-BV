---
title: "Analysis for final, prereg. sample of ARL-BV"
author: "Daniel P"
output: html_document
---

Set up
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(mc.cores=12)

path_to_project_directory <- "~/projects/ARL_bv/"
path_to_arl <- "~/projects/ARL/"
path_to_s22 <- "~/projects/spring_2022_study/"

stan_model_dir<- paste0(path_to_project_directory,"code/stan_models/final_samp_mods/")
model_out_dir <- paste0(path_to_project_directory,"output/results/stan_model_fits/final_samp_fits/")

#load custom functions
source(paste0(path_to_s22,"code/functions/s22_utilities.R")) 
source(paste0(path_to_s22,"code/functions/stan_utilities.R"))
source(paste0(path_to_arl,"code/functions/arl_utilities.R")) 
source(paste0(path_to_project_directory,"code/functions/arlbv_utilities.R")) 
source(paste0(path_to_s22,"code/functions/fit_stan_model.R"))

library(tidyverse)
library(cmdstanr)
library(loo)
library(bayesplot)
library(tidybayes)
library(lmerTest)
library(sigmoid)
```

Read in data, run through QC
```{r}
trials <- read.csv(paste0(path_to_project_directory,"analysis_data/trial_level_data_all_subs_2023-11-27_18_52_39.243708.csv"))
subs <- read.csv(paste0(path_to_project_directory,"analysis_data/sub_level_data_all_subs_2023-11-27_18_52_39.243708.csv"))

sub_hard_fail <- subs %>% filter(att_checks_passed < 5 |
                                 valrat_skipped_percent > .1 |
                                 late_percent > .2 |
                                 consecutive_late_choices > 9 |
                                 trials_completed < 144 |
                                 sd_valrat < .05 |
                                 percent_left > .8 |
                                 percent_left < .2 |
                                 percent_left_b1 < .1 |
                                 percent_left_b1 > .9 |
                                 percent_left_b2 < .1 |
                                 percent_left_b2 > .9 |
                                 percent_left_b3 < .1 |
                                 percent_left_b3 > .9 |
                                 answers_incorrect > 2)

#count the number of soft QC cutoffs each subject meets
subs$softs <- 0
for(i in 1:nrow(subs)){
  subs$softs[i] <- length(which(c(subs$att_checks_passed[i] == 5,subs$valrat_skipped_percent[i] > .05, subs$late_percent[i] > .15,
                                  subs$sd_valrat[i] < .1, subs$percent_left[i] > .75, subs$percent_left[i] < .25,
                                  subs$percent_left_b1[i] > .85, subs$percent_left_b1[i] < .15, subs$percent_left_b2[i] > .85, 
                                  subs$percent_left_b2[i] < .15,subs$percent_left_b3[i] > .85, subs$percent_left_b3[i] < .15,subs$answers_incorrect[i] > 1)))
}

sub_soft_fail <- filter(subs,softs >= 2) #identify those who meet more than 2 soft cutoffs

subs_to_exclude <- unique(c(sub_hard_fail$id,sub_soft_fail$id)) #get subjects who failed either set of criteria

#clean data
trials <- trials %>% filter(!(id %in% subs_to_exclude)) %>% filter(choice != "late") 
subs <- subs %>% filter(!(id %in% subs_to_exclude))

length(subs_to_exclude)/(nrow(subs)+length(subs_to_exclude)) #get percent excluded
```

Data transformations
```{r}
trials <- add_sub_indices(trials) #add subject indices to the df. These will match the indices used in Stan.
trials <- add_probe_number(trials,newcol="rat_number",val_col="valrat_z") #add rating number

#add a column with completed trial numbers - the trial indices if you ignore late trials. These will match the "t" trial numbers used in the Stan models
trials <- do.call(rbind,by(trials,trials$sub_index,add_trials_nl))
trials$overall_trial_nl <- 1:nrow(trials) #get the overall trial number ignoring late trials and collapsing across subjects

#Create indices from 1:n_f for each fractal image. To do this, first create a mini-df with one column having all the fA_img values and the other two
#columns having indices for fA and fB. This assumes that every fA_img is paired with a unique fB_img.
f_index_df <- data.frame(fA_img = unique(trials$fA_img),fA_ix = 1:length(unique(trials$fA_img)),fB_ix = (1:length(unique(trials$fA_img))+length(unique(trials$fA_img))))
trials <- left_join(trials,f_index_df,by="fA_img")

#get the chosen fractal index
trials <- trials %>% mutate(chosen_frac = ifelse(choice == "fA",fA_ix,fB_ix))
trials <- trials %>% mutate(unchosen_frac = ifelse(choice == "fA",fB_ix,fA_ix))
```

Some additional data review

```{r}
dr_vars <- c("valreg_rsq","bv_b","fres_b","fres_bv_ratio")
plot_sub_level_vars(subs,dr_vars,path_to_project_directory,type="dr") #create and save plot grids
```

```{r}
subs$age <- as.numeric(subs$age)
age_no_skips <- filter(subs,age < 900)
plot_hist(age_no_skips,"age") 
```

```{r}
sum(subs$gender=="Male")
```

```{r}
sum(subs$gender=="Female")
```

```{r}
sum(subs$gender=="Non-binary")
```

```{r}
sub_age <- subs %>% filter(age < 80)
mean(sub_age$age)
sd(sub_age$age)
```

```{r}
trials$pair_pres_num <- rep(c(1:48),323*3)
percentage_plots(trials,"stay")
```

# Preregistered analyses

Fit first couple of models
```{r}
one_q <- fit_stan_model(stan_file=paste0(stan_model_dir,"one_q.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

Check diagnostics
```{r}
one_q$diagnostics
```

```{r}
two_q$diagnostics
```
Ok

Compare
```{r}
fsml_compare(one_q,two_q)
```
Looks like two_q fits better

Does this show up in the learning and forgetting rates?
```{r}
twoq_lrn_dc_draws <- get_draws("two_q",model_out_dir=model_out_dir,vars=c("lrn_fr_mu","lrn_nf_mu","dcy_fr_mu","dcy_nf_mu"))
mcmc_areas(
  lfsm_draws,
  area_method = "scaled height",
  prob = 0.5,
  prob_outer = .95, 
  point_est = "median"
)
```

```{r}
sigmoid(filt_sum(two_q$sum,"lrn_fr_mu")$median)
sigmoid(filt_sum(two_q$sum,"lrn_nf_mu")$median)
(sigmoid(twoq_lrn_dc_draws[,,"lrn_fr_mu"]) - sigmoid(twoq_lrn_dc_draws[,,"lrn_nf_mu"])) > 0
```

Indeed, the learning rate is higher for fres than no-fres trials, as predicted.

Fit winner with side bias
```{r}
two_q_sidebias <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_sidebias.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 4,
                                   n_t = 144)
```

Compare
```{r}
fsml_compare(two_q,two_q_sidebias)
```

Testing effects of reward and affect associations on choice in two_q model
```{r}
twoq_aff <- get_draws("two_q",vars=c("scd_aff_fr_sens_mu"))
quantile(twoq_aff,c(.025,.5,.975))
mean(twoq_aff > 0)

twoq_rew <- get_draws("two_q",vars=c("scd_rew_fr_sens_mu"))
quantile(twoq_rew,c(.025,.5,.975))
mean(twoq_rew > 0)
```

two_q_sidebias
```{r}
aff_twoq_side <- get_draws("two_q_sidebias",vars=c("scd_aff_fr_sens_mu"),model_out_dir=model_out_dir)
rew_twoq_side <- get_draws("two_q_sidebias",vars=c("scd_rew_fr_sens_mu"),model_out_dir=model_out_dir)

cat("Effect of reward associations on choice\n")
quantile(rew_twoq_side,c(.50,.025,.975))
round(mean(rew_twoq_side > 0) * 100,1)

cat("Effect of affect associations on choice\n")
quantile(aff_twoq_side,c(.50,.025,.975))
round(mean(aff_twoq_side > 0) * 100,1)
```


Now, trying a model with separate effects of model-predicted and residual valence on choice
```{r}
mod_resid_sep <- fit_stan_model(stan_file=paste0(stan_model_dir,"mod_resid_sep.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

Fits a bit better


# Hypothesis tests

Posterior for effect of affect on choice
```{r}
twoq_int_draws <- get_draws("two_q",model_out_dir=model_out_dir,vars=c("aff_fr_sens_mu","rew_fr_sens_mu","B_rew_fr_mu","B_rew_nf_mu","B_bv_fr_mu","B_auto_mu"))

mcmc_areas(
  twoq_int_draws,
  pars = "aff_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

Posterior for effect of reward on choice
```{r}
mcmc_areas(
  twoq_int_draws,
  pars = "rew_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

```{r}
two_q_scds <- get_draws("two_q",model_out_dir=model_out_dir,vars = c("scd_aff_fr_sens_mu","scd_rew_fr_sens_mu","scd_csens_mu"))
quantile(two_q_scds[,,"scd_aff_fr_sens_mu"],c(.025,.5,.975))
quantile(two_q_scds[,,"scd_rew_fr_sens_mu"],c(.025,.5,.975))
quantile(two_q_scds[,,"scd_csens_mu"],c(.025,.5,.975))
```

# Other model parameters

```{r}
two_q <- read_fsml("two_q",model_out_dir=model_out_dir)
view(filt_sum(two_q$sum,"mu"))
```

# Exploratory analyses

Predict RL using model-predicted affect, and residual affect, with a fuller affect model
```{r}
mod_aff_resid <- fit_stan_model(stan_file=paste0(stan_model_dir,"mod_aff_resid.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
mom_sens_draws <- get_draws("mod_aff_resid",model_out_dir=model_out_dir,vars=c("aff_fr_sens_mu","rew_fr_sens_mu","resid_fr_sens_mu"))
mcmc_areas(
  mom_sens_draws,
  pars = "aff_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

Is this a stronger effect than what we had before?
```{r}
twoq_int_draws <- get_draws("two_q",model_out_dir=model_out_dir,vars=c("aff_fr_sens_mu"))

m_vs_rat <- mom_sens_draws[,,"aff_fr_sens_mu"] - twoq_int_draws[,,"aff_fr_sens_mu"]
quantile(m_vs_rat,probs=c(.01,.025,.05,.5,.95,.975,.99))

m_vs_resid <- mom_sens_draws[,,"aff_fr_sens_mu"] - mom_sens_draws[,,"resid_fr_sens_mu"]
quantile(m_vs_resid,probs=c(.01,.025,.05,.5,.95,.975,.99))
```


```{r}
mcmc_areas(
  mom_sens_draws,
  pars = "rew_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

```{r}
mcmc_areas(
  mom_sens_draws,
  pars = "resid_fr_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```
```{r}
filt_sum(mod_aff_resid$sum,"mu")
```

calculate R squared for valence prediction from this
```{r}
```


Momentum model
```{r}
mom_rat <- fit_stan_model(stan_file=paste0(stan_model_dir,"mom_rat.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
mom_terms <- get_draws("mom_rat",model_out_dir=model_out_dir,vars=c("x_sens_mu","aff_fr_sens_mu","B_auto_mu"))
mcmc_areas(
  mom_terms,
  pars = "x_sens_mu",
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
total_mom <- mom_terms[,,"x_sens_mu"] + mom_terms[,,"aff_fr_sens_mu"]*mom_terms[,,"B_auto_mu"]
mcmc_areas(
  total_mom,
  area_method = "scaled height",
  prob = 0.9,
  prob_outer = .99, 
  point_est = "median"
)
```

Breaking down effects of affect rating into trial-related, nuisance variation, and residual
```{r}
breakdown <- fit_stan_model(stan_file=paste0(stan_model_dir,"breakdown.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144,
                                   iter_sampling = 6000)
```

```{r}
filt_sum(breakdown$sum,"mu")
filt_sum(mod_aff_resid$sum,"mu")
```
Looks like it was worthwhile to break the result down like this - it resulted in much stronger estimates for aff_fres and allowed nuisance to be what it is.
```{r}
fsml_compare(breakdown,mod_aff_resid)
```

```{r}
breakdown_ma_sens <- get_draws("breakdown",vars=c("scd_ma_fr_sens_mu"))
quantile(breakdown_ma_sens,probs=c(.025,.5,.975))
breakdown_rew_sens <- get_draws("breakdown",vars=c("scd_rew_fr_sens_mu"))
quantile(breakdown_rew_sens,probs=c(.025,.5,.975))
quantile(breakdown_ma_sens - breakdown_rew_sens,probs=c(.025,.5,.975))
mean(breakdown_ma_sens > breakdown_rew_sens)
breakdown_resid_sens <- get_draws("breakdown",vars=c("scd_resid_fr_sens_mu"))
quantile(breakdown_resid_sens,probs=c(.025,.5,.975))

```



```{r}
final <- fit_stan_model(stan_file=paste0(stan_model_dir,"final.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144,
                                   skip=c("check_csvs","loo"),
                                   iter_sampling = 6000)
```

```{r}
final_Bbv <- get_draws("final",vars=c("B_bv_fr_mu"))
final_Brew <- get_draws("final",vars=c("B_rew_fr_mu"))
final_rew <- get_draws("final",vars=c("scd_rew_fr_sens_mu"))
final_aff <- get_draws("final",vars=c("scd_aff_fr_sens_mu"))
final_resid <- get_draws("final",vars=c("scd_resid_fr_sens_mu"))
val_mod_rsq <- get_draws("final",vars=c("val_mod_rsq"))
```

```{r}
trials_fres <- filter(trials,show_fres==1)
final_Bbv_scd <- sd(trials_fres$box_val)*final_Bbv
quantile(final_Bbv_scd,c(.025,.5,.975))
mean(final_Bbv_scd < 1)
```

```{r}
final_Brew_scd <- sd(trials_fres$out)*final_Brew
quantile(final_Brew_scd,c(.025,.5,.975))
mean(final_Brew_scd > 1)
```


```{r}
quantile(final_aff-final_rew,c(.025,.5,.975))
mean(final_aff > final_rew)
quantile(final_aff,c(.025,.5,.975))
mean(final_aff > 0)
quantile(final_rew,c(.025,.5,.975))
mean(final_rew > 0)
```

```{r}
quantile(final_resid,c(.025,.5,.975))
mean(final_resid > 0)

quantile(final_aff-final_resid,c(.025,.5,.975))
mean(final_aff > final_resid)
```

getting r-squared value
```{r}
median(val_mod_rsq)
```


# Regressing valence and reward onto choice


```{r}
trials_fres <- filter(trials,show_fres == 1)
trials_fres_scale <- trials_fres %>% mutate(valrat_z=scale(valrat_z),out=scale(out))
stay_fit <- lmer(stay ~ valrat_z + out + (valrat_z + out | sub_index),trials_fres_scale)
summary(stay_fit)
confint(stay_fit, parm = c("valrat_z","out"))
```
```{r}
trials_fres_lags <- add_lag_cols(trials_fres,c("valrat_z","out"),lags=c(1:4))
```

```{r}
trials_diffs <- do.call(rbind,by(trials,trials$sub_index,sub_past_diffs))

diff_reg_fit_1 <- lmer(stay ~ valrat_z_diff_1 + out_diff_1 + (valrat_z_diff_1 + out_diff_1 | sub_index),trials_diffs)
diff_reg_fit_1 <- lm(stay ~ valrat_z_diff_1 + out_diff_1,trials_diffs)

summary(diff_reg_fit_1)

diff_reg_fit_3 <- lmer(stay ~ valrat_z_diff_1 + out_diff_1 + valrat_z_diff_2 + out_diff_2 + valrat_z_diff_3 + out_diff_3 +
                              (valrat_z_diff_1 + out_diff_1 + valrat_z_diff_2 + out_diff_2 + valrat_z_diff_3 + out_diff_3 | sub_index),trials_diffs)

diff_reg_fit_3 <- lm(stay ~ valrat_z_diff_1 + out_diff_1 + valrat_z_diff_2 + out_diff_2 + valrat_z_diff_3 + out_diff_3,trials_diffs)
summary(diff_reg_fit_3)$coefficients
coef_values <- coefficients[2:4, 1]
```
```{r}
library(car)
linearHypothesis(diff_reg_fit_3,"valrat_z_diff_1 - valrat_z_diff_2 = 0")
linearHypothesis(diff_reg_fit_3,"valrat_z_diff_1 - valrat_z_diff_3 = 0")
```

#Utility vs. reinforcement test

```{r}
flt_pe <- fit_stan_model(stan_file=paste0(stan_model_dir,"flt_pe.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

```{r}
flt <- read_fsml("final_ls_test",model_out_dir=model_out_dir)
```

```{r}
flt_pe$loo
```

```{r}
fsml_compare(flt_pe,flt)
```
No real difference here

Testing with modeled affect only
```{r}
mod_aff <- fit_stan_model(stan_file=paste0(stan_model_dir,"mod_aff.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

```{r}
mod_aff_pe <- fit_stan_model(stan_file=paste0(stan_model_dir,"mod_aff_pe.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

```{r}
fsml_compare(mod_aff,mod_aff_pe)
```
Still no meaningful difference.


Testing if BV impacts choice more as utility or reinforcement
```{r}
rew_bv <- fit_stan_model(stan_file=paste0(stan_model_dir,"rew_bv.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

```{r}
rew_bv_pe <- fit_stan_model(stan_file=paste0(stan_model_dir,"rew_bv_pe.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

```{r}
fsml_compare(rew_bv,rew_bv_pe)
```

```{r}
view(filt_sum(rew_bv_pe$sum,"mu"))
```
No difference here either


Now doing the same thing but with r-BV instead of just BV
```{r}
rew_min_bv <- fit_stan_model(stan_file=paste0(stan_model_dir,"rew_min_bv.stan"),
                             model_out_dir=model_out_dir,
                             raw_data=trials,
                             study = "arlbv",
                             chains = 3,
                             n_t = 144)
```

```{r}
rew_min_bv_pe <- fit_stan_model(stan_file=paste0(stan_model_dir,"rew_min_bv_pe.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
fsml_compare(rew_min_bv,rew_min_bv_pe)
```
Still no difference.

Finally, testing with raw valence ratings.
```{r}
two_q_pe <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_pe.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
two_q_pe <- read_fsml("two_q_pe",model_out_dir=model_out_dir)
```

```{r}
two_q <- get_loo_from_csvs(files=c("/Users/dp/projects/ARL_bv/output/results/stan_model_fits/final_samp_fits/two_q/two_q-202406271907-2-82bfa0.csv", "/Users/dp/projects/ARL_bv/output/results/stan_model_fits/final_samp_fits/two_q/two_q-202406271907-3-82bfa0.csv", "/Users/dp/projects/ARL_bv/output/results/stan_model_fits/final_samp_fits/two_q/two_q-202406271907-1-82bfa0.csv"),variables=c("choice_lik","affect_lik"))
loo_compare(two_q,two_q_pe$loo)
```

```{r}
two_q_pe_comp <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_pe_comp.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

This is a confusing result. Why is the SE so big when calculating loo this way, but not with the original loo calculation with 4 chains?
I think you need to just start over with this - it's unclear what you did here.
```{r}
two_q_sidebias_pe <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_sidebias_pe.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 4,
                                   n_t = 144)
```

```{r}
fsml_compare(two_q_sidebias_pe,two_q_sidebias)
```
Okay, so when using raw ratings, the model fits better assuming that valence is utility. Now, re-testing without
Q-learning.

```{r}
two_q_sidebias_pe_aonly <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_sidebias_pe_aonly.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
two_q_sidebias_aonly <- fit_stan_model(stan_file=paste0(stan_model_dir,"two_q_sidebias_aonly.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   chains = 3,
                                   n_t = 144)
```

```{r}
fsml_compare(two_q_sidebias_pe_aonly,two_q_sidebias_aonly)
```


# Get parameter estimates from final model not reported in the main article
```{r}
# Get info on the posterior distribution for all relevant parameters
unreported_params <- "rew_nf_sens_mu|aff_nf_sens_mu|resid_nf_sens_mu|csens_mu|dcy_fr_mu|dcy_nf_mu|lrn_fr_mu|ls_bias_mu|lrn_nf_mu|lrn_c_mu|B_0_mu|B_rew_nf_mu|B_q_fr_mu|B_q_nf_mu|B_pwqd_fr_mu|B_pwqd_nf_mu|B_auto_mu"
up_tab <- select(filt_sum(mod_resid$sum,unreported_params),variable,median,q5,q95)
up_tab[2,3,4] <- as.numeric(up_tab[2,3,4])

# Replace choice predictors with scaled effects
scd_rew_nf_sens_mu <- get_draws("mod_resid",vars=c("scd_rew_nf_sens_mu"), model_out_dir = model_out_dir)
up_tab[1,2:4] <- t(quantile(scd_rew_nf_sens_mu,c(.50,.05,.95)))
scd_aff_nf_sens_mu <- get_draws("mod_resid",vars=c("scd_aff_nf_sens_mu"), model_out_dir = model_out_dir)
up_tab[2,2:4] <- t(quantile(scd_aff_nf_sens_mu,c(.50,.05,.95)))
scd_resid_nf_sens_mu <- get_draws("mod_resid",vars=c("scd_resid_nf_sens_mu"), model_out_dir = model_out_dir)
up_tab[3,2:4] <- t(quantile(scd_resid_nf_sens_mu,c(.50,.05,.95)))
scd_csens_mu <- get_draws("mod_resid",vars=c("scd_csens_mu"), model_out_dir = model_out_dir)
up_tab[5,2:4] <- t(quantile(scd_csens_mu,c(.50,.05,.95)))

up_tab[6:10,2:4] <- sigmoid(up_tab[6:10,2:4]) # Transform learning/decay rates to their original scales

up_tab[,c("median","q5","q95")] <- round(up_tab[,c("median","q5","q95")],2)
write.csv(up_tab,paste0(fig_dir,"unrep_params_s3.csv")) # This CSV will be converted into a figure
```

# Reanalysis

After re-doing the model comparison between the one_q, two_q, and two_q_sidebias models, I realized that two_q_sidebias actually fit better. So you need to rerun the final model with sidebias, and redo the stats accordingly.
```{r}
final_ls <- fit_stan_model(stan_file=paste0(stan_model_dir,"final_ls.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144,
                                   skip=c("check_csvs","loo"),
                                   iter_sampling = 6000)
```

Fitting at 1000 iterations so it can be compared to two_q_sidebias, and so it finishes running quicker
```{r}
final_ls_test <- fit_stan_model(stan_file=paste0(stan_model_dir,"final_ls_test.stan"),
                                   model_out_dir=model_out_dir,
                                   raw_data=trials,
                                   study = "arlbv",
                                   n_t = 144)
```

```{r}
fsml_compare(final_ls_test,two_q_sidebias)
```
```{r}
scd_aff_fr_sens_mu <- get_draws("final_ls",vars=c("scd_aff_fr_sens_mu"),model_out_dir=model_out_dir)
scd_rew_fr_sens_mu <- get_draws("final_ls",vars=c("scd_rew_fr_sens_mu"),model_out_dir=model_out_dir)
quantile(scd_aff_fr_sens_mu-scd_rew_fr_sens_mu,c(.025,.5,.975))
mean(scd_aff_fr_sens_mu > scd_rew_fr_sens_mu)
```


Get results from model with raw valence ratings
```{r}
prereg_aff <- get_draws("two_q_sidebias",vars=c("scd_aff_fr_sens_mu"),model_out_dir=model_out_dir)
quantile(prereg_aff,c(.025,.5,.975))
mean(prereg_aff > 0)
prereg_rew <- get_draws("two_q_sidebias",vars=c("scd_rew_fr_sens_mu"),model_out_dir=model_out_dir)
quantile(prereg_rew,c(.025,.5,.975))
mean(prereg_rew > 0)          
```

test the effects of Q and A using a model - two_q_sidebias - that updates A with raw valence ratings.
```{r}
aff_twoq_side <- get_draws("two_q_sidebias",vars=c("scd_aff_fr_sens_mu"),model_out_dir=model_out_dir)
rew_twoq_side <- get_draws("two_q_sidebias",vars=c("scd_rew_fr_sens_mu"),model_out_dir=model_out_dir)

cat("Effect of reward associations on choice\n")
quantile(rew_twoq_side,c(.50,.025,.975))
round(mean(rew_twoq_side > 0) * 100,1)

cat("Effect of affect associations on choice\n")
quantile(aff_twoq_side,c(.50,.025,.975))
round(mean(aff_twoq_side > 0) * 100,1)
```
